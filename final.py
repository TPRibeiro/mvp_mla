# -*- coding: utf-8 -*-
"""Final

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aK_gF3Z6EoLZ1_WHwhUPYxzuUZ3wjs-_

# A obesidade em dados.

# Introdu√ß√£o

Organiza√ß√£o Mundial da Sa√∫de (OMS) define obesidade como o ac√∫mulo excessivo de gordura corporal que pode prejudicar a sa√∫de (WHO, 2024), e √© considerada uma doen√ßa cr√¥nica que afeta pessoas de todas as idades e grupos sociais. A obesidade tamb√©m pode levar √† outras doen√ßas, como diabetes tipo 2, doen√ßas cardiovasculares, hipertens√£o, acidente vascular cerebral e v√°rios tipos de c√¢ncer.

Indiv√≠duos em risco s√£o orientados por profissionais de sa√∫de a comer menos calorias e se exercitar mais, frequentemente usando limites de √≠ndice de massa corporal (IMC) para triagem e para orientar o progresso e o progn√≥stico.

A OMS considera uma pessoa obesa quando seu √çndice de Massa Corporal (IMC) √© igual ou superior a 30 kg/m¬≤. O IMC √© calculado dividindo o peso da pessoa pela altura ao quadrado.

De acordo com a Organiza√ß√£o Pan-americana de Sa√∫de (PAHO), uma em cada oito pessoas, no mundo, vive com obesidade (2024), o que demonstra a import√¢ncia da preven√ß√£o e do controle da obesidade desde o in√≠cio da vida at√© a idade adulta.

A obesidade √© uma doen√ßa cr√¥nica complexa, mas as causas s√£o bem compreendidas, assim como as interven√ß√µes necess√°rias para conter a crise, que s√£o respaldadas por fortes evid√™ncias. No entanto, elas n√£o s√£o implementadas (PAHO, 2024).

Neste trabalho, estaremos estipulando o n√≠vel de obesidade para um indiv√≠duo em um problema de classifica√ß√£o multiclasse.

O objetivo seria n√£o apenas implementar as t√©cnicas aprendidas na _sprint_
 "Machine Learning & Analytics", mas tamb√©m facilitar a predi√ß√£o dos n√≠veis de obesidade e estimular a implementa√ß√£o de pol√≠ticas que possam combat√™-la.

O n√≠vel de obesidade ser√° definido como o valor _target_, e poder√° ser equivalente a: peso normal, sobrepeso n√≠vel I, sobrepeso n√≠vel II, obesidade tipo I, obesidade tipo II ou obesidade tipo III.

A fonte dos dados utilizados √© o trabalho intitulado "Dataset for estimation of obesity levels based on eating habits and physical condition in individuals from Colombia, Peru and Mexico." (PALECHOR & MANOTAS, 2019). Os dados foram obtidos em um arquivo .csv e cont√™m 17 atributos (colunas) e 2.111 registros (linhas). De acordo com os autores, 77% dos dados foram gerados sinteticamente usando a ferramenta Weka e o filtro SMOTE, e 23% dos dados foram coletados diretamente dos usu√°rios por meio de uma plataforma web.

Adicionalmente, os valores atribu√≠dos aos n√≠veis de obesidade (NObesity) se basearam no √çndice de Massa Corporal (IMC) do usu√°rio e na respectiva classifica√ß√£o utilizada pela OMS, que se d√° da seguinte forma:

* IMC < 18,5 kg/m¬≤ - baixo peso
* IMC > 18,5 at√© 24,9 kg/m¬≤ - peso adequado
* IMC ‚â• 25 at√© 29,9 kg/m¬≤ - sobrepeso
* IMC > 30,0 kg/m¬≤ at√© 34,9 kg/m¬≤ - obesidade grau 1
* IMC > 35 kg/m¬≤ at√© 39,9 kg/m¬≤ - obesidade grau 2
* IMC > 40 kg/m¬≤ - obesidade extrema

O dataset foi importado do Kaggle a partir de: https://www.kaggle.com/datasets/abdelrahman16/obesity-dataset; alguns valores foram legendados incorretamente no Kaggle, mas corrigidos aqui baseando-se no trabalho original.
"""

# Configura√ß√£o para n√£o exibir os warnings
import warnings
warnings.filterwarnings("ignore")

#Imports necess√°rios
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import kagglehub
path = kagglehub.dataset_download("abdelrahman16/obesity-dataset") # Download e local da vers√£o mais recente do dataset de n√≠veis de obesidade (https://www.kaggle.com/datasets/abdelrahman16/obesity-dataset?select=ObesityDataSet_raw_and_data_sinthetic.csv)
print("Local dos arquivos do dataset:", path)
csv_file = os.path.join(path, "ObesityDataSet_raw_and_data_sinthetic.csv") # para encontrar o arquivo CSV dentro do diret√≥rio acima
from sklearn.model_selection import train_test_split # para particionar em bases de treino e teste (holdout)
from sklearn.model_selection import KFold # para preparar os folds da valida√ß√£o cruzada
from sklearn.model_selection import cross_val_score # para executar a valida√ß√£o cruzada
from sklearn.metrics import accuracy_score # para a exibi√ß√£o da acur√°cia do modelo
from sklearn.neighbors import KNeighborsClassifier # algoritmo KNN
from sklearn.tree import DecisionTreeClassifier # algoritmo √Årvore de Classifica√ß√£o
from sklearn.naive_bayes import GaussianNB # algoritmo Naive Bayes
from sklearn.svm import SVC # algoritmo SVM
from sklearn.preprocessing import LabelEncoder # para transformar valores n√£o-num√©ricos em num√©ricos
from sklearn.impute import SimpleImputer # para tratar dados faltantes
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
print("Importa√ß√£o conclu√≠da com sucesso.")

"""Abaixo, se encontram as 17 colunas no arquivo importado do Kaggle, com suas respectivas descri√ß√µes:

**Vis√£o geral do dataset:**


*   üßë‚Äçü¶± Age: Idade do indiv√≠duo.
*   üöª Gender: G√™nero do indiv√≠duo.
*   üìè Height: Altura do indiv√≠duo.
*   ‚öñÔ∏è Weight: Peso do indiv√≠duo.
*   üçæ CALC: Frequ√™ncia do consumo de √°lcool.
*   üçü FAVC: Consumo frequente de comidas de alta caloria.
*   ü•ó FCVC: Frequ√™ncia de consumo de vegetais.
*   üçΩÔ∏è NCP: N√∫mero de refei√ß√µes principais.
*   üìì SCC: Monitoriza√ß√£o do consumo de calorias.
*   üö¨ SMOKE: Fumante.
*   üíß CH2O: Consumo di√°rio de √°gua.
*   üè† Family history of overweight: Hist√≥rico familiar de sobrepeso.
*   üèÉ FAF: Frequ√™ncia de atividade f√≠sica.
*   ‚åõ TUE: Tempo utilizando dispositivos tecnol√≥gicos.
*   üçï CAEC: Consumo de comida entre refei√ß√µes.
*   üöó MTRANS: M√©todo de transporte.
*   ‚öñÔ∏è NObeyesdad: N√≠vel de obesidade.

# Prepara√ß√£o dos dados
"""

obesidade = pd.read_csv(csv_file) # Leitura do arquivo CSV anteriormente importado
dataset = obesidade.copy() # Determina√ß√£o do dataset (c√≥pia para eventuais erros)
dataset = dataset.rename(columns={'NObeyesdad': 'target'}) # Renomear a coluna 'NObeyesdad', ou n√≠veis de obesidade, para 'target'
print("Dataset carregado com sucesso!", "\n\nPrimeiras linhas do dataset:")
dataset.head() # Mostra as primeiras linhas do dataset

"""Como pode ser observado na amostra do dataset na tabela gerada acima, este possui alguns valores n√£o-num√©ricos (aqui nomeados valores NaN, do ingl√™s _not a number_).

Assim, estes valores precisar√£o ser transformados em valores num√©ricos, para que os modelos de machine learning possam ler os dados com sucesso. Esta transforma√ß√£o ser√° realizada na etapa "prepara√ß√£o de dados 1" abaixo.

A t√≠tulo de controle, a quantifica√ß√£o de dados NaN e da transforma√ß√£o dos dados NaN para num√©ricos ser√° apresentada no fim do c√≥digo. √â esperado que haja 0 dados NaN no total.


---


O mapeamento dos valores bin√°rios se dar√° conforme o descrito abaixo:

Gender (g√™nero):
* Female: 0;
* Male: 1

FAVC (Consumo frequente de comidas de alta caloria);
SCC (Monitoriza√ß√£o do consumo cal√≥rico);
SMOKE (Fumante);
Family_history_with_overweight (Hist√≥rico familiar de sobrepeso):
* No: 0;
* Yes: 1

---

O mapeamento de valores n√£o-bin√°rios se dar√° de acordo com a legenda abaixo:

CAEC (Consumo de comida entre refei√ß√µes);
CALC (Consumo de √°lcool):

* No: 0,
* Sometimes: 1,
* Frequently: 2,
* Always: 3

MTRANS (m√©todo de transporte):

* Public Transportation: 0,
* Walking: 1,
* Automobile: 2,
* Motorbike: 3,
* Bike: 4

target (N√≠veis de obesidade):

* Normal weight: 0,
* Overweight level I: 1,
* Overweight level II: 2,
* Obesity type I: 3,
* Obesity type II: 4,
* Obesity type III: 5,
* Insufficient Weight': 6


"""

# Prepara√ß√£o dos dados 1


# Convertendo as colunas: 'Gender', 'favc', 'scc', 'smoke', 'family_history_with_overweight', para valores num√©ricos usando LabelEncoder
le = LabelEncoder()
dataset['Gender'] = le.fit_transform(dataset['Gender'])
dataset['FAVC'] = le.fit_transform(dataset['FAVC'])
dataset['SCC'] = le.fit_transform(dataset['SCC'])
dataset['SMOKE'] = le.fit_transform(dataset['SMOKE'])
dataset['family_history_with_overweight'] = le.fit_transform(dataset['family_history_with_overweight'])

# Convertendo para valores num√©ricos as colunas 'CAEC', 'MTRANS', 'target' e 'calc' com mapeamento ordinal
# E aplicando os mapeamentos √†s respectivas colunas.

#CAEC
CAEC_mapping = {
    'no': 0,
    'Sometimes': 1,
    'Frequently': 2,
    'Always': 3
}
dataset['CAEC'] = dataset['CAEC'].map(CAEC_mapping)

#MTRANS
MTRANS_mapping = {
    'Public_Transportation': 0,
    'Walking': 1,
    'Automobile': 2,
    'Motorbike': 3,
    'Bike': 4
}
dataset['MTRANS'] = dataset['MTRANS'].map(MTRANS_mapping)

#target
# Limpar espa√ßos extras e padronizar para min√∫sculas para evitar valores NaN
dataset['target_original'] = dataset['target']  # Salvar original para confer√™ncia (no caso de erros)
dataset['target'] = dataset['target'].str.strip().str.lower()

# Atualizar o mapeamento com os valores limpos
target_mapping = {
    'normal_weight': 0,
    'overweight_level_i': 1,
    'overweight_level_ii': 2,
    'obesity_type_i': 3,
    'obesity_type_ii': 4,
    'obesity_type_iii': 5,
    'insufficient_weight': 6
}
dataset['target'] = dataset['target'].map(target_mapping)

#CALC
CALC_mapping = {
    'no': 0,
    'Sometimes': 1,
    'Frequently': 2,
    'Always': 3
}
dataset['CALC'] = dataset['CALC'].map(CALC_mapping)

print('Verifica√ß√£o dos dados:\n')
# Verifica√ß√£o dos valores NaN: confirmando que todos os dados foram mapeados para num√©ricos com sucesso.
nan_counts = dataset.isna().sum()
print("Quantidade de valores NaN (not a number) em cada coluna:\n", nan_counts)

#Confirma√ß√£o dos valores √∫nicos dispon√≠veis para cada coluna: confirmando de que o mapeamento foi correto de valores n√£o-bin√°rios para num√©ricos.
print("\nConfirma√ß√£o dos valores √∫nicos dispon√≠veis em cada coluna (cujos valores n√£o-bin√°rios foram mapeados):")
print("Valores √∫nicos em CAEC:", dataset['CAEC'].unique())
print("Valores √∫nicos em MTRANS:", dataset['MTRANS'].unique())
print("Valores √∫nicos em target:", dataset['target'].unique())
print("Valores √∫nicos em CALC:", dataset['CALC'].unique())

"""O pr√≥ximo passo, a "prepara√ß√£o dos dados 2", ser√° preparar os dados para o treinamento e avalia√ß√£o do modelo de aprendizado de m√°quina. Este envolver√° dividir os dados, lidar com valores ausentes (devido ao aparecimento de um erro) e configurar a valida√ß√£o cruzada."""

#Prepara√ß√£o dos dados 2

test_size = 0.20 # tamanho do conjunto de teste
seed = 13 # semente aleat√≥ria

# Separa√ß√£o em bases de treino e teste
array = dataset.values
X = array[:,0:16] # atributos
y = array[:,16] # classe (target)

X_train, X_test, y_train, y_test = train_test_split(X, y,
    test_size=test_size, shuffle=True, random_state=seed, stratify=y) # holdout com estratifica√ß√£o

# Preenchendo valores faltantes com a moda e convertendo para int. Este passo foi necess√°rio devido a apari√ß√£o de um erro ao se utilizar o modelo KNN na etapa de modelagem, que n√£o estava conseguindo ler certos valores corretamente em "y".
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='most_frequent') # Usando a moda para dados categ√≥ricos
# Aplicando imputer para y_train e y_test separadamente
y_train = imputer.fit_transform(y_train.reshape(-1, 1)).ravel()
y_test = imputer.transform(y_test.reshape(-1, 1)).ravel()

# Explicitamente convertendo para tipo inteiro do Numpy, garantindo que as categorias dos n√≠veis de obesidade est√£o sendo representadas em n√∫meros inteiros.
y_train = y_train.astype(np.int64) # or np.int32
y_test = y_test.astype(np.int64) # or np.int32

# Par√¢metros e parti√ß√µes da valida√ß√£o cruzada
scoring = 'accuracy'
num_particoes = 10
kfold = StratifiedKFold(n_splits=num_particoes, shuffle=True, random_state=seed) # valida√ß√£o cruzada com estratifica√ß√£o

"""# Modelagem e treinamento

Agora que os dados foram preparados, diferentes modelos de machine learning ser√£o testados individualmente para que seja verificado qual deles apresenta o melhor desempenho na previs√£o dos n√≠veis de obesidade. No fim, um gr√°fico boxplot ser√° gerado, permitindo uma compara√ß√£o visual do desempenho de cada modelo.

A avalia√ß√£o foi realizada com valida√ß√£o cruzada, que garante uma avalia√ß√£o mais robusta e imparcial dos modelos, e assim, permitir√° a sele√ß√£o do modelo com melhor capacidade de generaliza√ß√£o para dados novos e desconhecidos.
"""

# Modelagem sem ensemble

# Definindo uma seed global para esta c√©lula de c√≥digo
np.random.seed(13)

# Listas para armazenar os modelos, os resultados e os nomes dos modelos
models = []
results = []
names = []

# Preparando os modelos e adicionando-os em uma lista
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC()))

# Avaliando um modelo por vez
for name, model in models:
  cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')
  results.append(cv_results)
  names.append(name)
  msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std()) # m√©dia e desvio padr√£o dos 10 resultados da valida√ß√£o cruzada
  print(msg)

# Boxplot de compara√ß√£o dos modelos
fig = plt.figure()
fig.suptitle('Compara√ß√£o da Acur√°cia dos Modelos')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
plt.show()

"""Tendo em vista os resultados acima, a √°rvore de decis√£o,  seria o modelo recomendado. Isso se deve, pois o CART (um algoritmo para a elabora√ß√£o de uma √Årvore de Decis√£o) apresentou os melhores resultados: cerca de 94% de acur√°cia m√©dia e desvio-padr√£o de aproximadamente 3%.

Verificaremos agora se este resultado pode ser melhorado com o uso de _ensemble_.

Os m√©todos _ensemble_ combinam v√°rios modelos de machine learning, que podem ser de diferentes algoritmos ou varia√ß√µes do mesmo, para criar um modelo final.
"""

# Modelagem

np.random.seed(13) # definindo uma semente global

# Lista que armazenar√° os modelos
models = []

# Criando os modelos e adicionando-os na lista de modelos
models.append(('LR', LogisticRegression(max_iter=200)))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC()))

# Definindo os par√¢metros do classificador base para o BaggingClassifier
base = DecisionTreeClassifier()
num_trees = 100
max_features = 3

# Criando os modelos para o VotingClassifier
bases = []
model1 = LogisticRegression(max_iter=200)
bases.append(('logistic', model1))
model2 = DecisionTreeClassifier()
bases.append(('cart', model2))
model3 = SVC()
bases.append(('svm', model3))

# Criando os ensembles e adicionando-os na lista de modelos
models.append(('Bagging', BaggingClassifier(estimator=base, n_estimators=num_trees)))
models.append(('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features)))
models.append(('ET', ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)))
models.append(('Ada', AdaBoostClassifier(n_estimators=num_trees)))
models.append(('GB', GradientBoostingClassifier(n_estimators=num_trees)))
models.append(('Voting', VotingClassifier(bases)))

# Listas para armazenar os resultados
results = []
names = []

# Avalia√ß√£o dos modelos
for name, model in models:
    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)
    results.append(cv_results)
    names.append(name)
    msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
    print(msg)

# Boxplot de compara√ß√£o dos modelos
fig = plt.figure(figsize=(15,10))
fig.suptitle('Compara√ß√£o da Acur√°cia dos Modelos')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
plt.show()

"""De acordo com o resultado acima, podemos determinar que o uso do m√©todo ensemble boosting, mais precisamente com o algoritmo Gradient Boosting (GB), foi o que obteve melhor resultado: cerca de 96% de acur√°cia m√©dia com um desvio-padr√£o de aproximadamente 1,5%.

O resultado do ensemble √© melhor que o do m√©todo base, que encontrou cerca de 94% de acur√°cia m√©dia e desvio-padr√£o de cerca de 3%

Agora, vamos repetir esse processo, utilizando a biblioteca Pipeline para criar e avaliar os modelos atrav√©s da valida√ß√£o cruzada com os dados padronizados e normalizados, e comparar o seu resultado com os modelos treinados com o dataset original.

O StandardScaler ser√° usado para padroniza√ß√£o dos dados. Ele transforma os dados subtraindo a m√©dia e dividindo pelo desvio padr√£o, resultando em uma distribui√ß√£o com m√©dia zero e vari√¢ncia unit√°ria.

O MinMaxScaler ser√° usado para normaliza√ß√£o dos dados. Ele escala os dados para um intervalo espec√≠fico, geralmente entre 0 e 1, subtraindo o valor m√≠nimo e dividindo pelo intervalo (m√°ximo - m√≠nimo).

Ao usar pipelines em conjunto com a valida√ß√£o cruzada, √© poss√≠vel garantir que o pr√©-processamento de dados (padroniza√ß√£o/normaliza√ß√£o) ser√° aplicado corretamente, evitando o data leakage e permitindo uma avalia√ß√£o mais realista do desempenho do modelo. Isso contribui para a constru√ß√£o de modelos mais robustos e generaliz√°veis.
"""

#Padroniza√ß√£o e normaliza√ß√£o dos dados

np.random.seed(13) # definindo uma semente global para este bloco

# Listas para armazenar os pipelines e os resultados para todas as vis√µes do dataset
pipelines = []
results = []
names = []


# Criando os elementos do pipeline

# Algoritmos que ser√£o utilizados
reg_log = ('LR', LogisticRegression(max_iter=200))
knn = ('KNN', KNeighborsClassifier())
cart = ('CART', DecisionTreeClassifier())
naive_bayes = ('NB', GaussianNB())
svm = ('SVM', SVC())
bagging = ('Bag', BaggingClassifier(estimator=base, n_estimators=num_trees))
random_forest = ('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features))
extra_trees = ('ET', ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features))
adaboost = ('Ada', AdaBoostClassifier(n_estimators=num_trees))
gradient_boosting = ('GB', GradientBoostingClassifier(n_estimators=num_trees))
voting = ('Voting', VotingClassifier(bases))

# Transforma√ß√µes que ser√£o utilizadas
standard_scaler = ('StandardScaler', StandardScaler()) #padroniza√ß√£o
min_max_scaler = ('MinMaxScaler', MinMaxScaler()) #normaliza√ß√£o


# Montando os pipelines

# Dataset original
pipelines.append(('LR-orig', Pipeline([reg_log])))
pipelines.append(('KNN-orig', Pipeline([knn])))
pipelines.append(('CART-orig', Pipeline([cart])))
pipelines.append(('NB-orig', Pipeline([naive_bayes])))
pipelines.append(('SVM-orig', Pipeline([svm])))
pipelines.append(('Bag-orig', Pipeline([bagging])))
pipelines.append(('RF-orig', Pipeline([random_forest])))
pipelines.append(('ET-orig', Pipeline([extra_trees])))
pipelines.append(('Ada-orig', Pipeline([adaboost])))
pipelines.append(('GB-orig', Pipeline([gradient_boosting])))
pipelines.append(('Vot-orig', Pipeline([voting])))

# Dataset Padronizado
pipelines.append(('LR-padr', Pipeline([standard_scaler, reg_log])))
pipelines.append(('KNN-padr', Pipeline([standard_scaler, knn])))
pipelines.append(('CART-padr', Pipeline([standard_scaler, cart])))
pipelines.append(('NB-padr', Pipeline([standard_scaler, naive_bayes])))
pipelines.append(('SVM-padr', Pipeline([standard_scaler, svm])))
pipelines.append(('Bag-padr', Pipeline([standard_scaler, bagging])))
pipelines.append(('RF-padr', Pipeline([standard_scaler, random_forest])))
pipelines.append(('ET-padr', Pipeline([standard_scaler, extra_trees])))
pipelines.append(('Ada-padr', Pipeline([standard_scaler, adaboost])))
pipelines.append(('GB-padr', Pipeline([standard_scaler, gradient_boosting])))
pipelines.append(('Vot-padr', Pipeline([standard_scaler, voting])))

# Dataset Normalizado
pipelines.append(('LR-norm', Pipeline([min_max_scaler, reg_log])))
pipelines.append(('KNN-norm', Pipeline([min_max_scaler, knn])))
pipelines.append(('CART-norm', Pipeline([min_max_scaler, cart])))
pipelines.append(('NB-norm', Pipeline([min_max_scaler, naive_bayes])))
pipelines.append(('SVM-norm', Pipeline([min_max_scaler, svm])))
pipelines.append(('Bag-norm', Pipeline([min_max_scaler, bagging])))
pipelines.append(('RF-norm', Pipeline([min_max_scaler, random_forest])))
pipelines.append(('ET-norm', Pipeline([min_max_scaler, extra_trees])))
pipelines.append(('Ada-norm', Pipeline([min_max_scaler, adaboost])))
pipelines.append(('GB-norm', Pipeline([min_max_scaler, gradient_boosting])))
pipelines.append(('Vot-norm', Pipeline([min_max_scaler, voting])))

print('Modelo: acur√°cia (desvio-padr√£o)')
# Executando os pipelines
for name, model in pipelines:
    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)
    results.append(cv_results)
    names.append(name)
    msg = "%s: %.3f (%.3f)" % (name, cv_results.mean(), cv_results.std()) # formatando para 3 casas decimais
    print(msg)

# Boxplot de compara√ß√£o dos modelos
fig = plt.figure(figsize=(25,6))
fig.suptitle('Compara√ß√£o dos Modelos - Dataset orginal, padronizado e normalizado')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names, rotation=90)
plt.show()

"""Ao avaliar os modelos, p√¥de-se determinar que o Gradient Boosting (GB) permaneceu com o melhor resultado. A padroniza√ß√£o e normaliza√ß√£o dos dados gerou um ligeiro aumento na acur√°cia dos testes, apesar de uma pequena diminui√ß√£o do desvio-padr√£o.

Embora o menor desvio padr√£o do GB-orig (original) seja atraente, a acur√°cia ligeiramente maior do GB-padr (padronizado) ou GB-norm (normalizado) os tornam as melhores escolhas.

Ainda assim, o desvio padr√£o do GB-padr foi de 0,017, enquanto o de GB-norm foi de 0,018.

Considerando esses fatores, GB-padr demonstrou o melhor equil√≠brio entre acur√°cia e estabilidade, tornando-o uma escolha mais robusta para a maioria dos cen√°rios.

A seguir, tentaremos novamente melhorar esse resultado, agora especificamente o do GB-padr, ao executar a otimiza√ß√£o dos hiperpar√¢metros do algoritmo Gradient Boosting. A t√©cnica utilizada foi a RandomizedSearchCV da biblioteca scikit-learn.

A Randomized Search amostra aleatoriamente um n√∫mero predefinido de combina√ß√µes de hiperpar√¢metros a partir de um espa√ßo de par√¢metros especificado. A escolha da Randomized Search (e seus valores) se deu com o intuito de deixar o c√≥digo mais r√°pido e leve, pois ao se tentar utilizar o Grid Search, o requerimento computacional e o tempo necess√°rio para rodar o c√≥digo foram significativamente maiores e n√£o justific√°veis para este trabalho.




"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.ensemble import GradientBoostingClassifier
import numpy as np

#Integrar o Pr√©-processamento no Pipeline
# Pipeline para GB-padr
pipeline_padr = Pipeline([
    ('scaler', StandardScaler()),
    ('gb', GradientBoostingClassifier())
])

# Grade de hiperpar√¢metros para GB-padr com ajuste do StandardScaler
param_dist_padr = {
    'scaler__with_mean': [True, False],
    'scaler__with_std': [True, False],
    'gb__n_estimators': [50, 100, 200],  # N√∫mero de √°rvores
    'gb__learning_rate': [0.01, 0.1, 0.2],  # Taxas de aprendizado
    'gb__max_depth': [3, 5, 7],  # Profundidade m√°xima
    'gb__subsample': [0.8, 1.0],  # Subamostragem para reduzir overfitting
    'gb__min_samples_split': [2, 5],  # M√≠nimo de amostras para dividir um n√≥
    'gb__min_samples_leaf': [1, 2]  # M√≠nimo de amostras em um n√≥ folha
}

# RandomizedSearchCV para explorar o espa√ßo de busca de forma mais eficiente
random_search_padr = RandomizedSearchCV(
    estimator=pipeline_padr,
    param_distributions=param_dist_padr,
    n_iter=15,  # N√∫mero de itera√ß√µes
    cv=5,  # N√∫mero de folds na valida√ß√£o cruzada
    scoring='accuracy',
    random_state=13,  # Para reprodutibilidade
    n_jobs=-1  # Utiliza todos os n√∫cleos do processador para paralelizar
)

# Treinamento do modelo
random_search_padr.fit(X_train, y_train)

# Imprimir a acur√°cia m√©dia do treino
cv_results = random_search_padr.cv_results_

# Melhores hiperpar√¢metros e acur√°cia
print("Melhores hiperpar√¢metros (GB-padr):", random_search_padr.best_params_)

# Acessando os resultados da valida√ß√£o cruzada
cv_results = random_search_padr.cv_results_
mean_accuracy = cv_results['mean_test_score'][random_search_padr.best_index_]
std_accuracy = cv_results['std_test_score'][random_search_padr.best_index_]

# Imprimindo a acur√°cia com o desvio-padr√£o (estimativa do desempenho esperado do modelo com os hiperpar√¢metros selecionados em dados n√£o vistos)
print(f"Acur√°cia m√©dia do treino (GB-padr): {mean_accuracy:.3f} (¬±{std_accuracy:.3f})")

# Avalia√ß√£o do modelo com o conjunto de testes

# Prepara√ß√£o do modelo
scaler = StandardScaler().fit(X_train) # ajuste do Scaler com o conjunto de treino
rescaledX = scaler.transform(X_train) # aplica√ß√£o da padroniza√ß√£o no conjunto de treino

# Extraindo apenas os par√¢metros de GB de best_params_
gb_params = {key[3:]: value for key, value in random_search_padr.best_params_.items() if key.startswith('gb__')}
# Removendo underlines ("_") das chaves (keys).
gb_params = {k.lstrip('_'): v for k, v in gb_params.items()}

model = GradientBoostingClassifier(**gb_params) # Usando apenas os par√¢metros GB
model.fit(rescaledX, y_train)

# Estimativa da acur√°cia no conjunto de teste
rescaledTestX = scaler.transform(X_test) # aplica√ß√£o da padroniza√ß√£o no conjunto de teste
predictions = model.predict(rescaledTestX)
print('Acur√°cia m√©dia do teste:', accuracy_score(y_test, predictions))

"""O modelo obteve aproximadamente 97% de acur√°cia no treino (utilizando-se a valida√ß√£o cruzada) e aproximadamente 97,6% de acur√°cia no teste no momento em que este trabalho foi escrito (tendo em vista a natureza aleat√≥ria do Randomized Search, √© poss√≠vel que os valores variem ligeiramente cada vez que o c√≥digo for rodado).

As acur√°cias no treino e no teste foram altas e pr√≥ximas, determinando um bom ajuste do modelo - ou seja, o modelo performa bem tanto com os dados de treino quanto os dados de teste, indicando que ele aprendeu a generalizar para dados novos sem estar muito ajustado aos dados de treino (aus√™ncia de underfitting ou overfitting).

O c√≥digo a seguir preparar√° todo o conjunto de dados, padronizando os atributos e garantindo que a vari√°vel target estar√° no formato correto. Em seguida, ele treinar√° o modelo final de machine learning nesses dados preparados, permitindo que ele fa√ßa previs√µes sobre novos dados.
"""

# Prepara√ß√£o do modelo com TODO o dataset
scaler = StandardScaler().fit(X) # ajuste do scaler com TODO o dataset
rescaledX = scaler.transform(X) # aplica√ß√£o da padroniza√ß√£o com TODO o dataset
y = y.astype(np.int64) # Convertendo y para tipo inteiro do Numpy, garantindo que as categorias dos n√≠veis de obesidade est√£o sendo representadas em n√∫meros inteiros.

model.fit(rescaledX, y)

"""# Resultados & Conclus√£o

Agora que o modelo foi finalizado, poderemos test√°-lo ao alimentar novos dados. Esses dados ser√£o fornecidos pelo usu√°rio e gerar√£o o resultado (o valor target), quando todos forem fornecidos corretamente.

Como os dados originais se encontram em ingl√™s, o c√≥digo inclui a tradu√ß√£o dos valores que ser√£o fornecidos pelo usu√°rio alvo em portugu√™s para o ingles. Legendas foram incluidas para todos os passos e espera-se que o resultado seja fornecido sem erros desde que as informa√ß√µes sejam inseridas corretamente.
"""

# Novos dados - os dados ser√£o um input do usu√°rio
data = {}
keys = ['Gender','Age','Height','Weight','FAF','NCP','CH2O','TUE','FCVC','FAVC','SCC','SMOKE','family_history_with_overweight','MTRANS','CAEC', 'CALC',]
numerical_attributes_printed = False
yes_no_values_printed = False
frequency_printed = False

# Adquirir o valor dos dados a partir do usu√°rio para cada key definida acima.
for key in keys:
    if key == 'Gender':
        while True:
            value = input(f"Vamos come√ßar com nossas perguntas! \nEste teste tem o potencial de predizer o seu n√≠vel de obesidade. Por favor, leia todas as quest√µes cuidadosamente.\n\nDigite o valor para {key}. Qual √© o seu g√™nero? (F/M): ").strip().lower()
            if value in ['m', 'f']:
                data[key] = 1 if value == 'm' else 0  # Mapeia para valores num√©ricos
                break
            else:
                print("Valor inv√°lido. Por favor, insira 'M' para masculino ou 'F' para feminino.")

    elif key in ['CAEC', 'CALC']:
        if not frequency_printed:  #Checa se j√° ocorreu a impress√£o ("print") da legenda, para evitar a repeti√ß√£o da legenda a cada inst√¢ncia.
            print("\nA seguir, as perguntas ir√£o lhe pedir a frequ√™ncia de CAEC e CALC. Considere:\n\nCAEC: Consumo de comida entre refei√ß√µes. Com que frequ√™ncia voc√™ come entre as refei√ß√µes principais?\nCALC: Consumo de √°lcool. Com que frequ√™ncia voc√™ consome bebidas alc√≥licas?\n")
            frequency_printed = True  # Determina o valor para True, impedindo aqui a repeti√ß√£o da legenda.
        while True:
            value = input(f"Digite o valor para {key} (nunca/√†s vezes/frequentemente/sempre):").strip().lower()

            # Tradu√ß√£o do input em portugu√™s (idioma do usu√°rio alvo) para o ingl√™s (a l√≠ngua que os dados est√£o)
            if value == "nunca":
                translated_value = "no"
            elif value == "√†s vezes":
                translated_value = "sometimes"
            elif value == "frequentemente":
                translated_value = "frequently"
            elif value == "sempre":
                translated_value = "always"
            else:
                translated_value = None  # Para inputs inv√°lidos

            if translated_value in ['no', 'sometimes', 'frequently', 'always']:
                # Aplica os mapeamentos
                if key == 'CAEC':
                    data[key] = CAEC_mapping.get(translated_value)
                elif key == 'CALC':
                    data[key] = CALC_mapping.get(translated_value)
                break
            else:
                print("Valor inv√°lido. Por favor, digite 'nunca', '√†s vezes', 'frequentemente' ou 'sempre'.")
    elif key == 'MTRANS':
       while True:
           value = input(f"\nDigite o valor para {key}. Qual o m√©todo de transporte que costuma utilizar com mais frequ√™ncia? \nEscolha entre: transporte p√∫blico / caminhada / carro / moto / bicicleta:\n").strip().lower()
           # Tradu√ß√£o do PT para EN
           if value in ["transporte p√∫blico", "caminhada", "carro", "moto", "bicicleta"]:
               if value == "transporte p√∫blico":
                   translated_value = "public_transportation"
               elif value == "caminhada":
                   translated_value = "walking"
               elif value == "carro":
                    translated_value = "automobile"
               elif value == "moto":
                   translated_value = "motorbike"
               elif value == "bicicleta":
                   translated_value = "bike"

               data[key] = MTRANS_mapping.get(translated_value)  # Aplica o mapeamento do MTRANS
               break # Movido para dentro do if
           else:
               print("Valor inv√°lido. Por favor, digite 'transporte p√∫blico', 'caminhada', 'carro', 'moto', ou 'bicicleta'.")

    elif key in ['FAVC', 'SCC', 'SMOKE', 'family_history_with_overweight']:  # Lida com as quest√µes sim/n√£o
        if not yes_no_values_printed:  # Verifica se a legenda j√° foi "printed"
           print('\nOs pr√≥ximos valores requisitados s√£o perguntas onde dever√° responder "sim" ou "nao" ("n√£o" sem acento). Considere:\n\nFAVC: Consumo frequente de comidas altamente cal√≥ricas. Voc√™ consome comidas de alta caloria frequentemente? (ex.:fast food)\nSCC: Monitoriza√ß√£o cal√≥rica. Voc√™ monitora quantas calorias ingere diariamente?\nSMOKE: Fumante. Voc√™ fuma?\nfamily_history_with_overweight: Hist√≥rico familiar de sobrepeso. Algu√©m na sua fam√≠lia tem sobrepeso?\n')
           yes_no_values_printed = True
        while True:
            value = input(f"Insira o valor para {key} (sim/nao): ").strip().lower()
            if value in ['sim', 'nao']:
                data[key] = 1 if value == 'sim' else 0  # Mapeia para valores num√©ricos
                break
            else:
                print("Valor inv√°lido. Por favor, digite 'sim' ou 'nao' (sem acento).")

    else:  # Handle numerical attributes
        if not numerical_attributes_printed:  # Verifica se a legenda j√° foi "printed"
           print('\nA seguir, valores num√©ricos ser√£o requisitados. Considere:\n\nAge: Idade. Qual sua idade?\nHeight: Altura. Qual sua altura? (em metro). \nAten√ß√£o: Use ponto ao inv√©s de v√≠rgula! Ex: ao inv√©s de 1,70, digite 1.70\nWeight: Peso. Qual seu peso? (em kilogramas)\nFAF: Frequ√™ncia de atividade f√≠sica. Quantos dias por semana voc√™ pratica alguma atividade f√≠sica?\nNCP:Frequ√™ncia de refei√ß√µes. Quantas refei√ß√µes principais voc√™ faz por dia?\nCH2O: Consumo de √°gua. Quantos litros de √°gua voc√™ bebe por dia?\nTUE: Tempo utilizando dispositivos tecnol√≥gicos. Entre os valores 0 e 3, quanto tempo por dia passa nesses dispositivos?\n    Digite de acordo: 0 - para 0 √† 2 horas de uso; 1 - para 3 √† 5 horas; 3 - para 6 horas ou mais.\nFCVC: Consumo de vegetais. Entre os valores 1 e 3, com que frequ√™ncia consome vegetais com suas refei√ß√µes?\n    Digite: 1 - nunca; 2 - √†s vezes; 3 - sempre\n',)
           numerical_attributes_printed = True
        while True:
            try:
                value = float(input(f"Insira um valor para {key}: "))
                data[key] = value
                break
            except ValueError:
                print("Valor inv√°lido. Por favor, insira um valor num√©rico. Caso tenha recebido o erro em 'Height', certifique-se de que usou '.' ao inv√©s de ','")

entrada = pd.DataFrame([data], columns=keys)
print('\n\nCalculando...\n\n')
# Aplicando StandardScaler e imputando dentro de um Pipeline. Isso garante que as mesmas etapas de pr√©-processamento usadas durante o treinamento ser√£o aplicadas aos novos dados
from sklearn.pipeline import Pipeline
preprocessor = Pipeline([
    ('scaler', StandardScaler()),
    ('imputer', SimpleImputer(strategy='most_frequent'))  # Imputa ap√≥s o escalonamento
])

# Ajusta o pr√©-processador aos dados de treinamento originais
preprocessor.fit(X_train)

# Transforma os novos dados de entrada usando o pr√©-processador ajustado
X_entrada_processed = preprocessor.transform(entrada)

# Continua com a predi√ß√£o
prediction = model.predict(X_entrada_processed)[0]

# Mapeamento reverso para a vari√°vel 'target'
target_mapping_reverse = {v: k for k, v in target_mapping.items()}
predicted_target = target_mapping_reverse.get(prediction)

# Dicion√°rio para tradu√ß√£o
traducao_target = {
    'normal_weight': 'Peso normal',
    'overweight_level_i': 'Sobrepeso n√≠vel I',
    'overweight_level_ii': 'Sobrepeso n√≠vel II',
    'obesity_type_i': 'Obesidade tipo I',
    'obesity_type_ii': 'Obesidade tipo II',
    'obesity_type_iii': 'Obesidade tipo III',
    'insufficient_weight': 'Peso insuficiente'
}

# Tradu√ß√£o do resultado
predicted_target_pt = traducao_target.get(predicted_target, predicted_target)

print(f"N√≠vel de obesidade previsto: {predicted_target_pt}")

"""Neste trabalho enfrentamos um problema de classifica√ß√£o multiclasse e na resolu√ß√£o deste, foi determinado que o ensemble boosting, com o algoritmo Gradient Boosting padronizado, apresentou a melhor acur√°cia. O modelo posteriormente gerado tamb√©m apresentou um bom ajuste e performou conforme o esperado, gerando um resultado de acordo.

O IMC pode ser impreciso em sua medi√ß√£o da obesidade, pois possui diversas limita√ß√µes. Pode-se dizer que o IMC estima a adiposidade de forma grosseira, tendo em vista que m√∫sculos e ossos t√™m uma densidade maior do que a gordura (tecido adiposo), e algumas disposi√ß√µes de gordura s√£o piores do que outras. O IMC n√£o mede a sa√∫de funcional e a aptid√£o f√≠sica, e √© meramente uma medida antropom√©trica (WU _et al_, 2024).

Ainda assim, o IMC tem uma grande vantagem, que √© a facilidade de obten√ß√£o de um resultado. O √≠ndice n√£o envolve radia√ß√£o ou exames de imagem, avalia√ß√µes antropom√©tricas tecnicamente complexas ou consultas m√©dicas ou a necessidade de submeter a procedimentos cl√≠nicos ou exames laboratoriais.

Com a quantidade cada vez maior de dados dispon√≠veis relacionados √† obesidade, podemos desenvolver modelos de intelig√™ncia artificial que tamb√©m permitem um f√°cil uso e resultado, que podem ser utilizados em colabora√ß√£o com os dados do IMC.

O modelo desenvolvido neste trabalho pode ser utilizado com este intuito. Com os valores podendo ser diretamente fornecidos pelo usu√°rio, este se torna interativo e permite uma avalia√ß√£o facilitada, pois o resultado se torna imediatamente dispon√≠vel.

Assim, o uso do resultado do modelo, em conjunto com o IMC, poderia fornecer uma compreens√£o mais eficaz dos riscos que um paciente corre. Por exemplo, um paciente com um IMC que o colocasse em uma categoria X, mas que ao fornecer seus dados ao modelo fosse colocado em uma categoria Y de risco de obesidade mais alto, poderia ser estimulada a procurar um m√©dico. A combina√ß√£o dos resultados poderia tamb√©m ser utilizada para facilitar a implementa√ß√£o de pol√≠ticas p√∫blicas.

# Refer√™ncias

* Aulas da sprint: Machine Learning & Analytics
*    Palechor, F. M., & Manotas, A. H. (2019). Dataset for estimation of obesity levels based on eating habits and physical condition in individuals from Colombia, Peru and Mexico. Data in brief, 25, 104344. https://doi.org/10.1016/j.dib.2019.104344
*    Pan-American Health Organization. PAHO. (2024). Uma em cada oito
pessoas, no mundo, vive com obesidade. https://www.paho.org/pt/noticias/1-3-2024-uma-em-cada-oito-pessoas-no-mundo-vive-com-obesidade
* World Health Organization. WHO. (2024) Obesity and overweight. https://www.who.int/news-room/fact-sheets/detail/obesity-and-overweight
* Wu, Y., Li, D., & Vermund, S. H. (2024). Advantages and Limitations of the Body Mass Index (BMI) to Assess Adult Obesity. International journal of environmental research and public health, 21(6), 757. https://doi.org/10.3390/ijerph21060757
"""